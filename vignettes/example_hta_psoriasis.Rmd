---
title: "Example: Plaque Psoriasis HTA report"
output: rmarkdown::html_vignette
bibliography: ../inst/REFERENCES.bib
params:
  run_tests: FALSE
---

```{r, code=readLines("children/knitr_setup.R"), include=FALSE}
```

```{r, eval = FALSE}
library(multinma)
options(mc.cores = parallel::detectCores())
```
```{r setup, echo = FALSE}
library(multinma)
nc <- switch(tolower(Sys.getenv("_R_CHECK_LIMIT_CORES_")), 
             "true" =, "warn" = 2, 
             parallel::detectCores())
options(mc.cores = nc)
```

This vignette describes the analysis of treatments for moderate-to-severe plaque psoriasis from an HTA report [@Woolacott2006], replicating the analysis in NICE Technical Support Document 2 [@TSD2].
The data are available in this package as `hta_psoriasis`:
```{r}
head(hta_psoriasis)
```

Outcomes are ordered multinomial success/failure to achieve 50%, 75%, or 90% reduction in symptoms on the Psoriasis Area and Severity Index (PASI) scale. 
Some studies report ordered outcomes at all three cutpoints, others only one or two:
```{r}
dplyr::filter(hta_psoriasis, studyc %in% c("Elewski", "Gordon", "ACD2058g", "Altmeyer"))
```

Here, the outcome counts are given as "exclusive" counts. 
That is, for a study reporting all outcomes (e.g. Elewski), the counts represent the categories 50 < PASI < 75, 75 < PASI < 90, and 90 < PASI < 100, and the corresponding columns are named by the lower end of the interval.
\footnote{The alternative is "inclusive" counts, which would represent the overlapping categories PASI > 50, PASI > 70, and PASI > 90.}
Missing values are used where studies only report a subset of the outcomes.
For a study reporting only two outcomes, say PASI50 and PASI75 as in Gordon, the counts represent the categories 50 < PASI < 75 and 75 < PASI < 100.
For a study reporting only one outcome, say PASI70 as in Altmeyer, the count represents 70 < PASI < 100.
We also need the count for the lowest category (i.e. no higher outcomes achieved), which is equal to the sample size minus the counts in the other observed categories.

### Setting up the network
We begin by setting up the network.
We have arm-level ordered multinomial count data, so we use the function `set_agd_arm()`.
The function `multi()` helps us to specify the ordered outcomes correctly.
```{r}
pso_net <- set_agd_arm(hta_psoriasis, 
                       study = paste(studyc, year), 
                       trt = trtc, 
                       r = multi(r0 = sample_size - rowSums(cbind(PASI50, PASI75, PASI90), na.rm = TRUE), 
                                 PASI50, PASI75, PASI90,
                                 inclusive = FALSE, 
                                 type = "ordered"))
pso_net
```

Plot the network structure.
```{r hta_psoriasis_network_plot}
plot(pso_net, weight_edges = TRUE, weight_nodes = TRUE) + 
  # Nudge the legend over
  ggplot2::theme(legend.box.spacing = ggplot2::unit(0.75, "in"))
```

### Meta-analysis models
We fit both fixed effect (FE) and random effects (RE) models.

#### Fixed effect meta-analysis
First, we fit a fixed effect model using the `nma()` function with `trt_effects = "fixed"`, using a probit link function `link = "probit"`.
We use $\mathrm{N}(0, 10^2)$ prior distributions for the treatment effects $d_k$, and $\mathrm{N}(0, 100^2)$ prior distributions for the study-specific intercepts $\mu_j$.
We can examine the range of parameter values implied by these prior distributions with the `summary()` method:
```{r}
summary(normal(scale = 10))
summary(normal(scale = 100))
```

We also need to specify prior distributions for the latent cutpoints $c_\textrm{PASI75}$ and $c_\textrm{PASI90}$ on the underlying scale - here the PASI standardised mean difference due to the probit link (the cutpoint $c_\textrm{PASI50}=0$).
To make these easier to reason about, we actually specify priors on the *differences* between adjacent cutpoints, e.g. $c_\textrm{PASI90} - c_\textrm{PASI75}$ and $c_\textrm{PASI75} - c_\textrm{PASI50}$.
These can be given any positive-valued prior distribution, and Stan will automatically impose the necessary ordering constraints behind the scenes.
We choose to give these implicit flat priors `flat()`.
\footnote{The `flat()` prior is a special case where no prior information is added to the model, resulting in an implicit flat uniform prior distribution over the entire support for a parameter. This will be an improper prior if the parameter is unbounded, and is not generally advised unless the parameters are strongly identified. See the [Stan user's guide](https://mc-stan.org/docs/stan-users-guide/some-differences-in-the-statistical-models-that-are-allowed.html) for more details.}

The model is fitted using the `nma()` function.
```{r}
pso_fit_FE <- nma(pso_net, 
                  trt_effects = "fixed",
                  link = "probit",
                  prior_intercept = normal(scale = 100),
                  prior_trt = normal(scale = 10),
                  prior_aux = flat())
```

Basic parameter summaries are given by the `print()` method:
```{r}
pso_fit_FE
```

---
Note: the treatment effects are the *opposite sign* to those in TSD 2 [@TSD2].
This is because we parameterise the linear predictor as $\mu_j + d_k + c_m$, rather than $\mu_j + d_k - c_m$.
The interpretation here thus follows that of a standard binomial probit (or logit) regression; SMDs (or log ORs) greater than zero mean that the treatment increases the probability of an event compared to the comparator (and less than zero mean a reduction in probability).
Here higher outcomes are positive, and all of the active treatments are estimated to increase the response (i.e. a greater reduction) on the PASI scale compared to the network reference (supportive care).
---

By default, summaries of the study-specific intercepts $\mu_j$ are hidden, but could be examined by changing the `pars` argument:
```{r, eval=FALSE}
# Not run
print(pso_fit_FE, pars = c("d", "mu", "cc"))
```

The prior and posterior distributions can be compared visually using the `plot_prior_posterior()` function:
```{r pso_FE_pp_plot}
plot_prior_posterior(pso_fit_FE)
```

Focusing specifically on the cutpoints we see that these are highly identified by the data, which is why the implicit flat priors work for these parameters.
```{r pso_FE_pp_cutpoint_plot}
plot_prior_posterior(pso_fit_FE, prior = "aux")
```

#### Random effects meta-analysis
We now fit a random effects model using the `nma()` function with `trt_effects = "random"`.
Again, we use $\mathrm{N}(0, 10^2)$ prior distributions for the treatment effects $d_k$, $\mathrm{N}(0, 100^2)$ prior distributions for the study-specific intercepts $\mu_j$, implicit flat prior distributions for the latent cutpoints, and we additionally use a $\textrm{half-N}(5^2)$ prior for the heterogeneity standard deviation $\tau$.
We can examine the range of parameter values implied by these prior distributions with the `summary()` method:
```{r}
summary(normal(scale = 10))
summary(normal(scale = 100))
summary(half_normal(scale = 5))
```

Fitting the RE model
```{r, warning=FALSE}
pso_fit_RE <- nma(pso_net, 
                  trt_effects = "random",
                  link = "probit",
                  prior_intercept = normal(scale = 100),
                  prior_trt = normal(scale = 10),
                  prior_aux = flat(),
                  prior_het = half_normal(scale = 5))
```

Basic parameter summaries are given by the `print()` method:
```{r}
pso_fit_RE
```

By default, summaries of the study-specific intercepts $\mu_j$ and study-specific relative effects $\delta_{jk}$ are hidden, but could be examined by changing the `pars` argument:
```{r, eval=FALSE}
# Not run
print(pso_fit_RE, pars = c("d", "cc", "mu", "delta"))
```

The prior and posterior distributions can be compared visually using the `plot_prior_posterior()` function:
```{r pso_RE_pp_plot}
plot_prior_posterior(pso_fit_RE, prior = c("trt", "aux", "het"))
```


#### Model comparison
Model fit can be checked using the `dic()` function:
```{r}
(dic_FE <- dic(pso_fit_FE))
```
```{r}
(dic_RE <- dic(pso_fit_RE))
```

The random effects model has a lower DIC and the residual deviance is closer to the number of data points, so is preferred in this case.

We can also examine the residual deviance contributions with the corresponding `plot()` method.
```{r pso_FE_resdev_plot}
plot(dic_FE)
```

```{r pso_RE_resdev_plot}
plot(dic_RE)
```

Most data points are fit well, with posterior mean residual deviances close to the degrees of freedom.
The Meffert 1997 study has a substantially higher residual deviance contribution, which could be investigated further to see why this study appears to be an outlier.

### Further results
@TSD2 produce absolute predictions of probability of achieving responses at each PASI cutoff, assuming a Normal distribution for the baseline probit probability of PASI50 response on supportive care with mean $-1.097$ and precision $123$.
We can replicate these results using the `predict()` method.
The `baseline` argument takes a `distr()` distribution object, with which we specify the corresponding Normal distribution.
We set `type = "response"` to produce predicted probabilities (`type = "link"` would produce predicted probit probabilities).
```{r pso_pred_FE, fig.height = 2}
pred_FE <- predict(pso_fit_FE, 
                   baseline = distr(qnorm, mean = -1.097, sd = 123^-0.5), 
                   type = "response")
pred_FE
plot(pred_FE)
```
```{r pso_pred_RE, fig.height = 2}
pred_RE <- predict(pso_fit_RE, 
                   baseline = distr(qnorm, mean = -1.097, sd = 123^-0.5), 
                   type = "response")
pred_RE
plot(pred_RE)
```

We can modify the plots using standard `ggplot2` functions.
For example, to plot the cutpoints together with a colour coding (instead of split into facets):
```{r pso_pred_RE_colour, fig.height = 2}
library(ggplot2)
plot(pred_RE) +
  facet_null() +
  aes(colour = Category) +
  scale_colour_brewer(palette = "Blues")
```

If the `baseline` argument is omitted, predicted probabilities will be produced for every study in the network based on their estimated baseline probit probability $\mu_j$.

Treatment rankings, rank probabilities, and cumulative rank probabilities can also be produced.
We set `lower_better = FALSE` since higher outcome categories are better (the outcomes are positive).
```{r hta_psoriasis_ranks, fig.height=3}
(pso_ranks <- posterior_ranks(pso_fit_RE, lower_better = FALSE))
plot(pso_ranks)
```
```{r hta_psoriasis_rankprobs}
(pso_rankprobs <- posterior_rank_probs(pso_fit_RE, lower_better = FALSE))
plot(pso_rankprobs)
```
```{r hta_psoriasis_cumrankprobs}
(pso_cumrankprobs <- posterior_rank_probs(pso_fit_RE, lower_better = FALSE, cumulative = TRUE))
plot(pso_cumrankprobs)
```

## References

```{r hta_psoriasis_tests, include=FALSE, eval=params$run_tests}
#--- Test against TSD 2 results ---
library(testthat)
library(dplyr)

tol <- 0.05
tol_dic <- 0.1

```

